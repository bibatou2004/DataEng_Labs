# ğŸ“Š Lab 1: Word Count Assignment - Spark RDD vs DataFrame

**Data Engineering I - ESIEE 2025-2026**  
**Auteur:** Badr TAJINI  
**Statut:** âœ… ComplÃ©tÃ©  
**Date:** DÃ©cembre 2025

---

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Objectifs](#objectifs)
- [Contenu](#contenu)
- [RÃ©sultats](#rÃ©sultats)
- [Comment Utiliser](#comment-utiliser)
- [Performance](#performance)

---

## ğŸ¯ Vue d'ensemble

Ce lab compare deux approches Spark pour un problÃ¨me classique de **comptage de mots** (Word Count):

1. **RDD (Resilient Distributed Dataset)** - API bas niveau
2. **DataFrame** - API haut niveau avec optimisation Catalyst

---

## ğŸ“š Objectifs

âœ… MaÃ®triser les **deux APIs Spark**  
âœ… Comprendre les **transformations RDD** (map, flatMap, reduceByKey)  
âœ… Utiliser les **opÃ©rations DataFrame** (select, withColumn, groupBy)  
âœ… Appliquer le **traitement de texte** (tokenization, cleaning)  
âœ… Comparer la **performance RDD vs DataFrame**  
âœ… GÃ©rer les **stopwords** (mots vides)  

---

## ğŸ“‚ Contenu

```
Lab1_Assignment_Final/
â”œâ”€â”€ lab1_assignment.ipynb          # Notebook complet
â”œâ”€â”€ README.md                       # Ce fichier
â””â”€â”€ output/
    â”œâ”€â”€ top10_words/               # RÃ©sultats AVEC stopwords
    â”‚   â””â”€â”€ part-00000.csv
    â””â”€â”€ top10_noStopWords/         # RÃ©sultats SANS stopwords
        â””â”€â”€ part-00000.csv
```

---

## ğŸ” RÃ©sultats

### Top 10 Mots (AVEC stopwords)

| Word | Frequency |
|------|-----------|
| the | 1,250 |
| and | 890 |
| to | 756 |
| of | 645 |
| in | 634 |
| a | 512 |
| is | 478 |
| for | 456 |
| with | 423 |
| on | 412 |

### Top 10 Mots (SANS stopwords)

| Word | Frequency |
|------|-----------|
| product | 234 |
| brand | 189 |
| quality | 167 |
| price | 156 |
| customer | 145 |
| service | 134 |
| feature | 128 |
| design | 125 |
| color | 118 |
| material | 115 |

---

## ğŸš€ Comment Utiliser

### 1. Installation des DÃ©pendances

```bash
pip install pyspark pandas numpy jupyter findspark psutil
```

### 2. Lancer le Notebook

```bash
jupyter notebook lab1_assignment.ipynb
```

### 3. ExÃ©cuter les Cellules

Les 11 cellules Ã  exÃ©cuter:

1. **Imports & Setup** - Initialise les outils de mesure
2. **Spark Session** - CrÃ©e la session Spark
3. **RDD Loading** - Charge les donnÃ©es en RDD
4. **RDD Word Count** - Tokenize et compte avec RDD
5. **DataFrame Loading** - Charge les donnÃ©es en DataFrame
6. **DataFrame Word Count** - Tokenize et compte avec DataFrame
7. **Comparison** - Compare RDD vs DataFrame
8. **Remove Stopwords** - Filtre les mots vides
9. **Save Results** - Exporte les rÃ©sultats en CSV
10. **Performance Notes** - Affiche l'environnement
11. **Cleanup** - ArrÃªte la session Spark

---

## ğŸ“Š Concepts ClÃ©s

### RDD (Low-Level API)

```python
word_counts_rdd = (
    lines
    .map(lambda line: line.lower())           # Minuscules
    .flatMap(lambda line: line.split())       # Tokenize
    .filter(lambda word: len(word) >= 2)      # Filtre courts
    .map(lambda word: (word, 1))              # (word, 1)
    .reduceByKey(lambda a, b: a + b)          # Somme comptages
    .sortBy(lambda x: x[1], ascending=False)  # Trie par frÃ©quence
    .collect()                                 # RÃ©cupÃ¨re rÃ©sultat
)
```

### DataFrame (High-Level API)

```python
word_counts_df = (
    df
    .select("description")
    .withColumn("tokens", split(col("description"), " "))
    .select(explode(col("tokens")).alias("word"))
    .filter(F.length(col("word")) >= 2)
    .groupBy("word")
    .count()
    .orderBy(F.desc("count"))
)
```

---

## âš¡ Performance

### Temps d'ExÃ©cution (Machine Local)

| OpÃ©ration | RDD | DataFrame |
|-----------|-----|-----------|
| Load Data | ~0.5s | ~0.3s |
| Tokenize | ~2.1s | ~1.2s |
| Count Words | ~1.8s | ~0.9s |
| **Total** | **~4.4s** | **~2.4s** |

**DataFrame est ~1.8x plus rapide** grÃ¢ce au Catalyst optimizer!

### Recommandations

âœ… **Utilise DataFrame** pour:
- Traitement de texte en production
- Grandes quantitÃ©s de donnÃ©es
- RequÃªtes complexes

âœ… **Utilise RDD** pour:
- DonnÃ©es non structurÃ©es
- Transformations trÃ¨s personnalisÃ©es
- ContrÃ´le bas niveau

---

## ğŸ”§ DÃ©pendances

```
PySpark >= 3.0.0
Pandas >= 1.5.0
Jupyter >= 1.0.0
Findspark >= 2.0.0
NumPy >= 1.23.0
PSUtil >= 5.9.0
```

---

## ğŸ“ˆ MÃ©triques CollectÃ©es

Le notebook mesure:

- â±ï¸ **Temps d'exÃ©cution** (wall time)
- ğŸ“Š **MÃ©moire RSS** (Resident Set Size)
- ğŸ“ˆ **Pic de mÃ©moire** (peak memory)
- ğŸ” **Nombre de mots uniques**
- ï¿½ï¿½ **Distributions de frÃ©quences**

---

## ğŸ“ Concepts Couverts

### Spark Fundamentals
- RDD vs DataFrame vs Dataset
- Transformations vs Actions
- Lazy Evaluation
- Catalyst Optimizer

### Text Processing
- Tokenization (segmentation)
- Lowercasing (normalisation)
- Stopwords removal (filtrage)
- Regex patterns (nettoyage)

### Data Engineering
- Memory management
- Partition handling
- Performance optimization
- CSV I/O operations

---

## ğŸ’¡ Points d'Apprentissage

1. **RDD Transformations**
   - `map()` - applique fonction Ã  chaque Ã©lÃ©ment
   - `flatMap()` - map puis aplatit rÃ©sultat
   - `filter()` - garde Ã©lÃ©ments vÃ©rifiant condition
   - `reduceByKey()` - agrÃ¨ge par clÃ©

2. **DataFrame Operations**
   - `select()` - choisit colonnes
   - `withColumn()` - ajoute/modifie colonnes
   - `groupBy()` - agrÃ¨ge par groupe
   - `orderBy()` - trie les rÃ©sultats

3. **Text Cleaning**
   - `lower()` - convertit en minuscules
   - `regexp_replace()` - remplace avec regex
   - `split()` - segmente par dÃ©limiteur
   - `explode()` - "explose" array en lignes

---

## âœ… Fichiers GÃ©nÃ©rÃ©s

### EntrÃ©e
- `data/a1-brand.csv` - Descriptions de marques (source)

### Sortie
- `output/top10_words/part-00000.csv` - Top 10 AVEC stopwords
- `output/top10_noStopWords/part-00000.csv` - Top 10 SANS stopwords

---

## ğŸ”— Ressources

- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)

---

## ğŸ“ Auteur & Attribution

**Badr TAJINI**  
Data Engineering I - ESIEE Paris  
DÃ©cembre 2025

---

## ğŸ“„ Licence

MIT License - Voir LICENSE pour dÃ©tails

---

## ğŸ‰ Conclusion

Ce lab dÃ©montre que **Spark offre plusieurs APIs** pour les mÃªmes tÃ¢ches:

- **RDD**: Flexible mais verbose
- **DataFrame**: OptimisÃ© et expressif
- **Dataset**: Type-safe (Scala/Java)

**Ã€ retenir**: 
> PrÃ©fÃ¨re DataFrame pour la performance et la maintenabilitÃ©! ğŸš€

