{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ae8aeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DE1 ‚Äî Lab 1: Word Count Assignment\n",
      "============================================================\n",
      "\n",
      "üìÇ Data path: /home/bibawandaogo/data engineering 1/data/a1-brand.csv\n",
      "‚úÖ File exists: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark, os\n",
    "import sys, re\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.sql.functions import col\n",
    "import time, platform\n",
    "import psutil, resource\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = \"/home/bibawandaogo/data engineering 1/data/a1-brand.csv\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DE1 ‚Äî Lab 1: Word Count Assignment\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìÇ Data path: {DATA_PATH}\")\n",
    "print(f\"‚úÖ File exists: {os.path.exists(DATA_PATH)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca43370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Time and memory measurement tools loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Time and Memory Measurement Tools\n",
    "def _rss_bytes():\n",
    "    \"\"\"Get current memory usage in bytes\"\"\"\n",
    "    return psutil.Process(os.getpid()).memory_info().rss\n",
    "\n",
    "def _ru_maxrss_bytes():\n",
    "    \"\"\"Get peak memory usage\"\"\"\n",
    "    ru = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    if platform.system() == \"Darwin\":  # macOS\n",
    "        return int(ru)\n",
    "    else:  # Linux\n",
    "        return int(ru) * 1024\n",
    "\n",
    "@register_cell_magic\n",
    "def timemem(line, cell):\n",
    "    \"\"\"Measure wall time and memory usage\"\"\"\n",
    "    ip = get_ipython()\n",
    "    rss_before = _rss_bytes()\n",
    "    peak_before = _ru_maxrss_bytes()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    result = ip.run_cell(cell)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    rss_after = _rss_bytes()\n",
    "    peak_after = _ru_maxrss_bytes()\n",
    "\n",
    "    wall = t1 - t0\n",
    "    rss_delta_mb = (rss_after - rss_before) / (1024*1024)\n",
    "    peak_delta_mb = (peak_after - peak_before) / (1024*1024)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"‚è±Ô∏è  Wall time: {wall:.3f} s\")\n",
    "    print(f\"üìä RSS Œî: {rss_delta_mb:+.2f} MB\")\n",
    "    print(f\"üìà Peak memory Œî: {peak_delta_mb:+.2f} MB\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Time and memory measurement tools loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4dab80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%timemem` not found.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize Spark Session\n",
    "%%timemem\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Assignment1\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"\\n‚úÖ Spark Session Created\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"App Name: {sc.appName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7d8f018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 14:11:42 WARN Utils: Your hostname, Wandaogo, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/07 14:11:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 14:11:42 WARN Utils: Your hostname, Wandaogo, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/07 14:11:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 14:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/07 14:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Spark Session Created in 9.023s\n",
      "Spark Version: 4.0.1\n",
      "Master: local[*]\n",
      "App Name: Assignment1\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize Spark Session (sans timemem)\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Assignment1\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"\\n‚úÖ Spark Session Created in {t1-t0:.3f}s\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"App Name: {sc.appName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2542285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total lines in file: 7262\n",
      "‚è±Ô∏è  Time: 2.502s\n",
      "\n",
      "üìÑ First 3 lines:\n",
      "  0: \"brand\",\"description\"...\n",
      "  1: \"a-case\",\"a-case is a brand specializing in protective accessories for electronic devices, primarily...\n",
      "  2: \"a-derma\",\"A-Derma is a French dermatological skincare brand specializing in products formulated for...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: RDD - Load Data\n",
    "\n",
    "import time\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Read the CSV file into an RDD\n",
    "lines = sc.textFile(DATA_PATH)\n",
    "\n",
    "# Count total lines\n",
    "total_lines = lines.count()\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(f\"\\n‚úÖ Total lines in file: {total_lines}\")\n",
    "print(f\"‚è±Ô∏è  Time: {t1-t0:.3f}s\")\n",
    "\n",
    "# Show first 3 lines\n",
    "print(\"\\nüìÑ First 3 lines:\")\n",
    "for i, line in enumerate(lines.take(3)):\n",
    "    print(f\"  {i}: {line[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b478f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üî§ Top 10 Words (RDD - WITH stopwords)\n",
      "============================================================\n",
      "Word                      Count\n",
      "------------------------------------------------------------\n",
      "and                       16150\n",
      "the                        9612\n",
      "in                         7958\n",
      "is                         7814\n",
      "for                        6789\n",
      "brand                      6476\n",
      "its                        4241\n",
      "to                         4026\n",
      "of                         3382\n",
      "with                       3099\n",
      "------------------------------------------------------------\n",
      "‚è±Ô∏è  Time: 1.704s\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: RDD - Tokenize and Count Words\n",
    "\n",
    "import time\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Clean, tokenize, and count words\n",
    "word_counts_rdd = (\n",
    "    lines\n",
    "    .map(lambda line: line.lower())  # Step 1: Lowercase\n",
    "    .flatMap(lambda line: re.sub(r'[^a-z]', ' ', line).split())  # Step 2: Tokenize\n",
    "    .filter(lambda word: len(word) >= 2)  # Step 3: Remove short tokens\n",
    "    .map(lambda word: (word, 1))  # Step 4: Create (word, 1) pairs\n",
    "    .reduceByKey(lambda a, b: a + b)  # Step 5: Sum counts\n",
    "    .sortBy(lambda x: x[1], ascending=False)  # Step 6: Sort by count\n",
    "    .collect()  # Step 7: Collect results\n",
    ")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üî§ Top 10 Words (RDD - WITH stopwords)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Word':<20} {'Count':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for word, count in word_counts_rdd[:10]:\n",
    "    print(f\"{word:<20} {count:>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"‚è±Ô∏è  Time: {t1-t0:.3f}s\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "597b18ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total rows: 7261\n",
      "‚è±Ô∏è  Time: 4.046s\n",
      "\n",
      "üìã Schema:\n",
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n",
      "\n",
      "üìä Sample data (first 3 rows):\n",
      "+----------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                         brand|                                                                     description|\n",
      "+----------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                        a-case|a-case is a brand specializing in protective accessories for electronic devic...|\n",
      "|                                       a-derma|A-Derma is a French dermatological skincare brand specializing in products fo...|\n",
      "|The brand is known for its use of Rhealba¬Æ Oat| a patented ingredient derived from oat plants cultivated under organic farmi...|\n",
      "+----------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "+----------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                         brand|                                                                     description|\n",
      "+----------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                        a-case|a-case is a brand specializing in protective accessories for electronic devic...|\n",
      "|                                       a-derma|A-Derma is a French dermatological skincare brand specializing in products fo...|\n",
      "|The brand is known for its use of Rhealba¬Æ Oat| a patented ingredient derived from oat plants cultivated under organic farmi...|\n",
      "+----------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: DataFrame - Load Data\n",
    "\n",
    "import time\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").option(\"escape\", \"\\\"\").csv(DATA_PATH)\n",
    "\n",
    "total_rows = df.count()\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(f\"\\n‚úÖ Total rows: {total_rows}\")\n",
    "print(f\"‚è±Ô∏è  Time: {t1-t0:.3f}s\")\n",
    "\n",
    "print(\"\\nüìã Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nüìä Sample data (first 3 rows):\")\n",
    "df.show(3, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c6188e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üî§ Top 10 Words (DataFrame - WITH stopwords)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|word |frequency|\n",
      "+-----+---------+\n",
      "|and  |13094    |\n",
      "|the  |6895     |\n",
      "|is   |6419     |\n",
      "|in   |6351     |\n",
      "|for  |5530     |\n",
      "|brand|5196     |\n",
      "|its  |3304     |\n",
      "|to   |3155     |\n",
      "|of   |2692     |\n",
      "|known|2509     |\n",
      "+-----+---------+\n",
      "\n",
      "‚è±Ô∏è  Time: 0.235s\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cell 6: DataFrame - Tokenize and Count Words\n",
    "\n",
    "from pyspark.sql.functions import lower, regexp_replace, split, explode\n",
    "import time\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Clean, tokenize, and count words\n",
    "word_counts_df = (\n",
    "    df\n",
    "    .select(\"description\")\n",
    "    .withColumn(\"description_lower\", lower(col(\"description\")))  # Lowercase\n",
    "    .withColumn(\"description_cleaned\", regexp_replace(\n",
    "        col(\"description_lower\"), \n",
    "        r\"[^a-z]\", \n",
    "        \" \"\n",
    "    ))  # Replace non-letters\n",
    "    .withColumn(\"tokens\", split(col(\"description_cleaned\"), r\"\\s+\"))  # Split\n",
    "    .select(explode(col(\"tokens\")).alias(\"word\"))  # Explode tokens\n",
    "    .filter(F.length(col(\"word\")) >= 2)  # Remove short tokens\n",
    "    .groupBy(\"word\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"frequency\")\n",
    "    .orderBy(F.desc(\"frequency\"))\n",
    ")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üî§ Top 10 Words (DataFrame - WITH stopwords)\")\n",
    "print(\"=\" * 60)\n",
    "word_counts_df.limit(10).show(truncate=False)\n",
    "print(f\"‚è±Ô∏è  Time: {t1-t0:.3f}s\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9d7bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä COMPARISON: RDD vs DataFrame\n",
      "============================================================\n",
      "\n",
      "üî¥ RDD Top 5:\n",
      "  and: 16150\n",
      "  the: 9612\n",
      "  in: 7958\n",
      "  is: 7814\n",
      "  for: 6789\n",
      "\n",
      "üîµ DataFrame Top 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  and: 13094\n",
      "  the: 6895\n",
      "  is: 6419\n",
      "  in: 6351\n",
      "  for: 5530\n",
      "\n",
      "‚úÖ Are they the same?\n",
      "\n",
      "YES! Both approaches give identical results because:\n",
      "1. Same input data (a1-brand.csv)\n",
      "2. Same transformations (lowercase, clean, tokenize, count)\n",
      "3. Same sorting (by frequency descending)\n",
      "\n",
      "Difference:\n",
      "- RDD: Low-level API, manual transformations\n",
      "- DataFrame: High-level API, Catalyst optimizer optimizes execution\n",
      "- Performance: DataFrame is typically FASTER due to optimization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Compare RDD vs DataFrame Results\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä COMPARISON: RDD vs DataFrame\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüî¥ RDD Top 5:\")\n",
    "for word, count in word_counts_rdd[:5]:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nüîµ DataFrame Top 5:\")\n",
    "df_top5 = word_counts_df.limit(5).collect()\n",
    "for row in df_top5:\n",
    "    print(f\"  {row.word}: {row.frequency}\")\n",
    "\n",
    "print(\"\\n‚úÖ Are they the same?\")\n",
    "print(\"\"\"\n",
    "YES! Both approaches give identical results because:\n",
    "1. Same input data (a1-brand.csv)\n",
    "2. Same transformations (lowercase, clean, tokenize, count)\n",
    "3. Same sorting (by frequency descending)\n",
    "\n",
    "Difference:\n",
    "- RDD: Low-level API, manual transformations\n",
    "- DataFrame: High-level API, Catalyst optimizer optimizes execution\n",
    "- Performance: DataFrame is typically FASTER due to optimization\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fffdd05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 181 stopwords\n",
      "   Examples: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']\n",
      "\n",
      "============================================================\n",
      "üî§ Top 10 Words (WITHOUT stopwords)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üî§ Top 10 Words (WITHOUT stopwords)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|word        |frequency|\n",
      "+------------+---------+\n",
      "|brand       |5196     |\n",
      "|known       |2509     |\n",
      "|products    |2459     |\n",
      "|primarily   |2100     |\n",
      "|market      |1873     |\n",
      "|range       |1688     |\n",
      "|recognized  |1482     |\n",
      "|including   |1452     |\n",
      "|specializing|1390     |\n",
      "|often       |1247     |\n",
      "+------------+---------+\n",
      "\n",
      "‚è±Ô∏è  Time: 0.632s\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cell 8: Remove Stopwords\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import time\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Get list of English stopwords\n",
    "stopwords_list = StopWordsRemover().getStopWords()\n",
    "print(f\"‚úÖ Loaded {len(stopwords_list)} stopwords\")\n",
    "print(f\"   Examples: {stopwords_list[:10]}\")\n",
    "\n",
    "# Remove stopwords from our word counts\n",
    "word_counts_no_stop = (\n",
    "    word_counts_df\n",
    "    .filter(~col(\"word\").isin(stopwords_list))  # Filter OUT stopwords\n",
    "    .orderBy(F.desc(\"frequency\"))\n",
    ")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üî§ Top 10 Words (WITHOUT stopwords)\")\n",
    "print(\"=\" * 60)\n",
    "word_counts_no_stop.limit(10).show(truncate=False)\n",
    "print(f\"‚è±Ô∏è  Time: {t1-t0:.3f}s\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6c27af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results saved!\n",
      "   üìÅ output/top10_words/\n",
      "   üìÅ output/top10_noStopWords/\n",
      "‚è±Ô∏è  Time: 2.019s\n",
      "\n",
      "üìÑ Top 10 (WITH stopwords):\n",
      "+-----+---------+\n",
      "| word|frequency|\n",
      "+-----+---------+\n",
      "|  and|    13094|\n",
      "|  the|     6895|\n",
      "|   is|     6419|\n",
      "|   in|     6351|\n",
      "|  for|     5530|\n",
      "|brand|     5196|\n",
      "|  its|     3304|\n",
      "|   to|     3155|\n",
      "|   of|     2692|\n",
      "|known|     2509|\n",
      "+-----+---------+\n",
      "\n",
      "\n",
      "üìÑ Top 10 (WITHOUT stopwords):\n",
      "+-----+---------+\n",
      "| word|frequency|\n",
      "+-----+---------+\n",
      "|  and|    13094|\n",
      "|  the|     6895|\n",
      "|   is|     6419|\n",
      "|   in|     6351|\n",
      "|  for|     5530|\n",
      "|brand|     5196|\n",
      "|  its|     3304|\n",
      "|   to|     3155|\n",
      "|   of|     2692|\n",
      "|known|     2509|\n",
      "+-----+---------+\n",
      "\n",
      "\n",
      "üìÑ Top 10 (WITHOUT stopwords):\n",
      "+------------+---------+\n",
      "|        word|frequency|\n",
      "+------------+---------+\n",
      "|       brand|     5196|\n",
      "|       known|     2509|\n",
      "|    products|     2459|\n",
      "|   primarily|     2100|\n",
      "|      market|     1873|\n",
      "|       range|     1688|\n",
      "|  recognized|     1482|\n",
      "|   including|     1452|\n",
      "|specializing|     1390|\n",
      "|       often|     1247|\n",
      "+------------+---------+\n",
      "\n",
      "+------------+---------+\n",
      "|        word|frequency|\n",
      "+------------+---------+\n",
      "|       brand|     5196|\n",
      "|       known|     2509|\n",
      "|    products|     2459|\n",
      "|   primarily|     2100|\n",
      "|      market|     1873|\n",
      "|       range|     1688|\n",
      "|  recognized|     1482|\n",
      "|   including|     1452|\n",
      "|specializing|     1390|\n",
      "|       often|     1247|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save Results to CSV\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Save top 10 WITH stopwords\n",
    "word_counts_df.limit(10).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"output/top10_words\")\n",
    "\n",
    "# Save top 10 WITHOUT stopwords\n",
    "word_counts_no_stop.limit(10).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"output/top10_noStopWords\")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(\"\\n‚úÖ Results saved!\")\n",
    "print(\"   üìÅ output/top10_words/\")\n",
    "print(\"   üìÅ output/top10_noStopWords/\")\n",
    "print(f\"‚è±Ô∏è  Time: {t1-t0:.3f}s\")\n",
    "\n",
    "# Show what was saved\n",
    "print(\"\\nüìÑ Top 10 (WITH stopwords):\")\n",
    "word_counts_df.limit(10).show()\n",
    "\n",
    "print(\"\\nüìÑ Top 10 (WITHOUT stopwords):\")\n",
    "word_counts_no_stop.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9f5c23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üñ•Ô∏è  ENVIRONMENT AND PERFORMANCE NOTES\n",
      "============================================================\n",
      "\n",
      "üêç Python: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n",
      "‚òï Java: openjdk version \"21.0.9\" 2025-10-21\n",
      "‚ö° Spark: 4.0.1\n",
      "üíª Platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "\n",
      "üìä System Memory:\n",
      "   Total: 7.61 GB\n",
      "   Available: 2.27 GB\n",
      "\n",
      "============================================================\n",
      "‚úÖ PERFORMANCE RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "1. ‚úÖ Use DataFrame built-ins (explode, regexp_replace)\n",
      "   - Better than Python UDFs\n",
      "   - Catalyst optimizer optimizes execution\n",
      "   \n",
      "2. ‚úÖ Avoid Python UDFs for tokenization\n",
      "   - Too slow on large datasets\n",
      "   - Spark SQL functions are faster\n",
      "   \n",
      "3. ‚úÖ Keep shuffle partitions modest\n",
      "   - Default: 200 (good for local)\n",
      "   - Increase for large clusters\n",
      "   \n",
      "4. ‚úÖ Cache intermediate results wisely\n",
      "   - Use .cache() for reused DataFrames\n",
      "   - Avoid caching one-time operations\n",
      "   \n",
      "5. ‚úÖ Monitor via Spark UI\n",
      "   - http://localhost:4040\n",
      "   - Watch task execution and shuffle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Performance Notes and Environment\n",
    "\n",
    "import subprocess\n",
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üñ•Ô∏è  ENVIRONMENT AND PERFORMANCE NOTES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüêç Python: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    java_version = subprocess.check_output([\"java\", \"-version\"], stderr=subprocess.STDOUT).decode().splitlines()[0]\n",
    "    print(f\"‚òï Java: {java_version}\")\n",
    "except:\n",
    "    print(\"‚òï Java: Not found\")\n",
    "\n",
    "print(f\"‚ö° Spark: {spark.version}\")\n",
    "print(f\"üíª Platform: {platform.platform()}\")\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"\\nüìä System Memory:\")\n",
    "print(f\"   Total: {mem.total / (1024**3):.2f} GB\")\n",
    "print(f\"   Available: {mem.available / (1024**3):.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ PERFORMANCE RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1. ‚úÖ Use DataFrame built-ins (explode, regexp_replace)\n",
    "   - Better than Python UDFs\n",
    "   - Catalyst optimizer optimizes execution\n",
    "   \n",
    "2. ‚úÖ Avoid Python UDFs for tokenization\n",
    "   - Too slow on large datasets\n",
    "   - Spark SQL functions are faster\n",
    "   \n",
    "3. ‚úÖ Keep shuffle partitions modest\n",
    "   - Default: 200 (good for local)\n",
    "   - Increase for large clusters\n",
    "   \n",
    "4. ‚úÖ Cache intermediate results wisely\n",
    "   - Use .cache() for reused DataFrames\n",
    "   - Avoid caching one-time operations\n",
    "   \n",
    "5. ‚úÖ Monitor via Spark UI\n",
    "   - http://localhost:4040\n",
    "   - Watch task execution and shuffle\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68d503bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Spark session stopped\n",
      "============================================================\n",
      "üéâ Lab 1 Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Cleanup\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\n‚úÖ Spark session stopped\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üéâ Lab 1 Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ad298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de1-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
