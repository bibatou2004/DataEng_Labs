# ğŸš€ Lab 0: Introduction Ã  Apache Spark et PySpark

**Data Engineering I - ESIEE 2025-2026**  
**Auteur:** Badr TAJINI  
**Statut:** âœ… ComplÃ©tÃ©  
**DerniÃ¨re mise Ã  jour:** DÃ©cembre 2025

---

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Objectifs PÃ©dagogiques](#objectifs-pÃ©dagogiques)
- [Concepts ClÃ©s](#concepts-clÃ©s)
- [Architecture Spark](#architecture-spark)
- [Installation](#installation)
- [DÃ©marrage Rapide](#dÃ©marrage-rapide)
- [Exercices Pratiques](#exercices-pratiques)
- [Ressources](#ressources)

---

## ğŸ“Š Vue d'ensemble

**Lab 0** est une **introduction pratique** Ã  Apache Spark et PySpark, couvrant:

âœ… Configuration de l'environnement Spark  
âœ… CrÃ©ation et manipulation de DataFrames  
âœ… OpÃ©rations SQL et opÃ©rations RDD  
âœ… Transformations et Actions  
âœ… Optimisation des requÃªtes  
âœ… Sauvegarde et chargement de donnÃ©es  

---

## ğŸ¯ Objectifs PÃ©dagogiques

### 1. Comprendre l'Architecture Spark
- Master-Worker model
- DAG (Directed Acyclic Graph)
- ExÃ©cution distribuÃ©e

### 2. MaÃ®triser les DataFrames
- CrÃ©ation et chargement
- Transformations (filter, map, join)
- Actions (count, show, collect)

### 3. Optimiser les RequÃªtes
- Catalyst optimizer
- Query plans
- Partition pruning

### 4. Manipuler les DonnÃ©es
- CSV, JSON, Parquet
- SQL queries
- AgrÃ©gations

---

## ğŸ”§ Concepts ClÃ©s

### Spark vs PySpark

| Aspect | Spark | PySpark |
|--------|-------|---------|
| **Langage** | Scala (natif) | Python (wrapper) |
| **Performance** | Plus rapide | LÃ©gÃ¨rement plus lent |
| **FacilitÃ©** | ModÃ©rÃ© | TrÃ¨s facile |
| **Usage** | Production | Data Science |

### RDD vs DataFrame

| Concept | RDD | DataFrame |
|---------|-----|-----------|
| **Abstraction** | Low-level | High-level |
| **Structure** | Pas de schÃ©ma | SchÃ©ma typÃ© |
| **Performance** | Lent | Rapide (Catalyst) |
| **Usage** | DonnÃ©es non structurÃ©es | DonnÃ©es structurÃ©es |

### Transformations vs Actions

**Transformations** (Lazy):
```python
df.filter(F.col("age") > 18)          # Pas exÃ©cutÃ© immÃ©diatement
df.map(lambda x: x * 2)                # Pas exÃ©cutÃ© immÃ©diatement
df.join(other_df, "id")                # Pas exÃ©cutÃ© immÃ©diatement
```

**Actions** (Eager):
```python
df.count()                              # EXÃ‰CUTÃ‰ immÃ©diatement
df.show()                               # EXÃ‰CUTÃ‰ immÃ©diatement
df.collect()                            # EXÃ‰CUTÃ‰ immÃ©diatement
df.write.parquet(path)                  # EXÃ‰CUTÃ‰ immÃ©diatement
```

### DAG (Directed Acyclic Graph)

```
OpÃ©rations Spark â†’ DAG â†’ Spark Scheduler â†’ Tasks sur Workers

Exemple:
df.filter(age > 18)
  .groupBy("city")
  .count()
  
DAG:
[CSV File] â†’ [Filter] â†’ [GroupBy] â†’ [Count] â†’ [Result]
```

---

## ğŸ—ï¸ Architecture Spark

### Cluster Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SPARK CLUSTER                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚        Driver Program                â”‚        â”‚
â”‚  â”‚  (SparkContext, SparkSession)       â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚               â”‚                                  â”‚
â”‚               â”‚ Task Distribution                â”‚
â”‚               â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Executorâ”‚ Executorâ”‚ Executorâ”‚ (Workers)    â”‚
â”‚  â”‚ JVM 1   â”‚ JVM 2   â”‚ JVM N   â”‚              â”‚
â”‚  â”‚         â”‚         â”‚         â”‚              â”‚
â”‚  â”‚ Task 1  â”‚ Task 5  â”‚ Task 9  â”‚              â”‚
â”‚  â”‚ Task 2  â”‚ Task 6  â”‚ Task 10 â”‚              â”‚
â”‚  â”‚ Task 3  â”‚ Task 7  â”‚ Task 11 â”‚              â”‚
â”‚  â”‚ Task 4  â”‚ Task 8  â”‚ Task 12 â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚         â”‚         â”‚                    â”‚
â”‚       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â”‚
â”‚            â–¼         â–¼                         â”‚
â”‚         [Shuffle] [Results]                    â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Execution Model

```
1. DAG Construction (Lazy)
   â†“
2. Stage Breakdown (par Shuffle boundaries)
   â†“
3. Task Generation (une par partition)
   â†“
4. Scheduler Distribution (Ã  Workers)
   â†“
5. Execution (en parallÃ¨le)
   â†“
6. Result Collection (au Driver)
```

---

## ğŸ’» Installation

### PrÃ©requis

```bash
# Python 3.8+
python --version

# Java 8+
java -version

# Scala (optionnel)
scala -version
```

### Installation PySpark

```bash
# MÃ©thode 1: pip
pip install pyspark

# MÃ©thode 2: conda
conda install pyspark -c conda-forge

# MÃ©thode 3: source (advanced)
# TÃ©lÃ©charge depuis https://spark.apache.org/downloads.html
```

### VÃ©rification de l'Installation

```python
from pyspark.sql import SparkSession
from pyspark import __version__ as spark_version

print(f"PySpark version: {spark_version}")

spark = SparkSession.builder.appName("test").getOrCreate()
print(f"Spark {spark.version} initialisÃ© avec succÃ¨s!")

# ArrÃªte la session
spark.stop()
```

---

## ğŸš€ DÃ©marrage Rapide

### 1. CrÃ©er une SparkSession

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Mon App Spark") \
    .master("local[*]") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

print(f"âœ… Spark {spark.version} initialisÃ©")
```

### 2. CrÃ©er un DataFrame

```python
# Ã€ partir d'une liste
data = [
    ("Alice", 25, "Paris"),
    ("Bob", 30, "Lyon"),
    ("Charlie", 35, "Marseille")
]
columns = ["name", "age", "city"]

df = spark.createDataFrame(data, columns)
df.show()
```

**Output:**
```
+-------+---+---------+
|name   |age|city     |
+-------+---+---------+
|Alice  |25 |Paris    |
|Bob    |30 |Lyon     |
|Charlie|35 |Marseille|
+-------+---+---------+
```

### 3. Charger des DonnÃ©es

```python
# CSV
df_csv = spark.read.option("header", "true").csv("data.csv")

# Parquet
df_parquet = spark.read.parquet("data.parquet")

# JSON
df_json = spark.read.json("data.json")

# Affiche le schÃ©ma
df_csv.printSchema()
```

### 4. Manipuler les DonnÃ©es

```python
from pyspark.sql import functions as F

# Filter
df_age = df.filter(F.col("age") > 25)

# Select
df_names = df.select("name", "city")

# GroupBy
df_group = df.groupBy("city").count()

# Join
df_joined = df.join(other_df, "id")

# Affiche les rÃ©sultats
df_age.show()
```

### 5. Sauvegarder les DonnÃ©es

```python
# Parquet (recommandÃ©)
df.write.mode("overwrite").parquet("output/data.parquet")

# CSV
df.write.mode("overwrite").option("header", "true").csv("output/data.csv")

# JSON
df.write.mode("overwrite").json("output/data.json")
```

---

## ğŸ“š Exercices Pratiques

### Exercice 1: Chargement et Affichage

```python
# 1. CrÃ©e un DataFrame avec 5 utilisateurs
data = [
    (1, "Alice", 25),
    (2, "Bob", 30),
    (3, "Charlie", 35),
    (4, "Diana", 28),
    (5, "Eve", 32)
]
df = spark.createDataFrame(data, ["id", "name", "age"])

# 2. Affiche le contenu
df.show()

# 3. Affiche le schÃ©ma
df.printSchema()

# 4. Affiche les statistiques
df.describe().show()
```

**RÃ©sultat attendu:**
```
+---+-------+---+
|id |name   |age|
+---+-------+---+
|1  |Alice  |25 |
|2  |Bob    |30 |
|3  |Charlie|35 |
|4  |Diana  |28 |
|5  |Eve    |32 |
+---+-------+---+

root
 |-- id: long (nullable = true)
 |-- name: string (nullable = true)
 |-- age: long (nullable = true)

+-------+---+-------+
|summary|id |age    |
+-------+---+-------+
|count  |5  |5      |
|mean   |3.0|30.0   |
|stddev |1.58|4.12  |
|min    |1  |25     |
|max    |5  |35     |
+-------+---+-------+
```

---

### Exercice 2: Transformations de Base

```python
from pyspark.sql import functions as F

# Filtre: Ã¢ge > 28
df_filtered = df.filter(F.col("age") > 28)
print("Utilisateurs avec Ã¢ge > 28:")
df_filtered.show()

# Select: seulement nom et Ã¢ge
df_selected = df.select("name", "age")
print("Nom et Ã¢ge:")
df_selected.show()

# WithColumn: ajoute colonne
df_with_age_category = df.withColumn(
    "age_category",
    F.when(F.col("age") < 30, "Jeune").otherwise("Senior")
)
print("Avec catÃ©gorie d'Ã¢ge:")
df_with_age_category.show()

# Distinct: valeurs uniques
df_distinct = df.select("age").distinct()
print("Ã‚ges uniques:")
df_distinct.show()
```

---

### Exercice 3: AgrÃ©gations

```python
from pyspark.sql import functions as F

# Count par catÃ©gorie
df_with_category = df.withColumn(
    "category",
    F.when(F.col("age") < 30, "Young").otherwise("Old")
)

print("Nombre par catÃ©gorie:")
df_with_category.groupBy("category").count().show()

# Statistiques d'Ã¢ge
print("Statistiques d'Ã¢ge:")
df.agg(
    F.count("age").alias("count"),
    F.avg("age").alias("average"),
    F.min("age").alias("min"),
    F.max("age").alias("max"),
    F.stddev("age").alias("stddev")
).show()
```

---

### Exercice 4: RequÃªtes SQL

```python
# CrÃ©e une vue temporaire
df.createOrReplaceTempView("users")

# RequÃªte SQL
result = spark.sql("""
    SELECT name, age,
           CASE 
               WHEN age < 30 THEN 'Young'
               WHEN age < 40 THEN 'Middle'
               ELSE 'Senior'
           END as age_group
    FROM users
    WHERE age > 25
    ORDER BY age DESC
""")

print("RÃ©sultat SQL:")
result.show()
```

---

### Exercice 5: Joins

```python
# DataFrame 1: utilisateurs
users = spark.createDataFrame([
    (1, "Alice"),
    (2, "Bob"),
    (3, "Charlie")
], ["id", "name"])

# DataFrame 2: villes
cities = spark.createDataFrame([
    (1, "Paris"),
    (2, "Lyon"),
    (4, "Nice")
], ["id", "city"])

# Inner Join (seulement matches)
print("Inner Join:")
users.join(cities, "id", "inner").show()
# Output: Alice-Paris, Bob-Lyon

# Left Join (tous les users)
print("\nLeft Join:")
users.join(cities, "id", "left").show()
# Output: Alice-Paris, Bob-Lyon, Charlie-NULL

# Full Outer Join (tout)
print("\nFull Outer Join:")
users.join(cities, "id", "outer").show()
# Output: Alice-Paris, Bob-Lyon, Charlie-NULL, NULL-Nice
```

---

## ğŸ“Š Optimisation des RequÃªtes

### Catalyst Optimizer

Spark optimise automatiquement les requÃªtes:

```python
# RequÃªte non optimisÃ©e (explicite)
df1 = spark.read.csv("big_file.csv")
df2 = df1.filter(F.col("age") > 25)
df3 = df2.select("name")
result = df3.collect()

# Spark convertit internement en:
# 1. Lit le CSV
# 2. Filtre sur age > 25 (Predicate Pushdown)
# 3. SÃ©lectionne seulement 'name' (Column Pruning)
# 4. Collecte les rÃ©sultats

# RÃ©sultat: Lecture + Filtrage + Projection = Optimale
```

### Plans d'ExÃ©cution

```python
# Affiche le plan d'exÃ©cution
df.filter(F.col("age") > 25).explain(mode="formatted")

# Output:
# â”€ Filter
#    â”œâ”€ Condition: age > 25
#    â””â”€ Read CSV
#       â””â”€ Column Pruning: [id, name, age]
```

---

## ğŸ” Debugging et Monitoring

### Logs

```python
import logging

# Configure les logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Starting Spark application")
logger.debug(f"DataFrame count: {df.count()}")
logger.warning("Large dataset, consider partitioning")
```

### Spark UI

```python
# Le Spark UI est disponible Ã :
# http://localhost:4040

# Affiche les stages, tasks, et performance
# Accessible pendant et aprÃ¨s l'exÃ©cution
```

---

## ğŸ“ Structure du Projet

```
Lab0_Introduction/
â”‚
â”œâ”€â”€ ğŸ““ notebooks/
â”‚   â”œâ”€â”€ 01_Introduction.ipynb
â”‚   â”œâ”€â”€ 02_DataFrames.ipynb
â”‚   â”œâ”€â”€ 03_Transformations.ipynb
â”‚   â”œâ”€â”€ 04_SQL.ipynb
â”‚   â”œâ”€â”€ 05_Optimizations.ipynb
â”‚   â””â”€â”€ 06_Exercises.ipynb
â”‚
â”œâ”€â”€ ğŸ“Š donnees/
â”‚   â”œâ”€â”€ entrees/
â”‚   â”‚   â”œâ”€â”€ users.csv
â”‚   â”‚   â”œâ”€â”€ products.csv
â”‚   â”‚   â””â”€â”€ orders.json
â”‚   â”‚
â”‚   â””â”€â”€ sorties/
â”‚       â”œâ”€â”€ results.parquet
â”‚       â””â”€â”€ summary.csv
â”‚
â”œâ”€â”€ ğŸ“š docs/
â”‚   â”œâ”€â”€ CONCEPTS.md
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ OPTIMIZATION.md
â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_examples.sh
â”‚   â””â”€â”€ setup.sh
â”‚
â”œâ”€â”€ proof/
â”‚   â”œâ”€â”€ exercise_outputs.txt
â”‚   â””â”€â”€ performance_metrics.csv
â”‚
â”œâ”€â”€ README.md (EN FRANÃ‡AIS)
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ğŸ“¦ DÃ©pendances

```
pyspark>=3.0.0
pandas>=1.5.0
numpy>=1.23.0
jupyter>=1.0.0
findspark>=2.0.0
```

Installation:
```bash
pip install -r requirements.txt
```

---

## ğŸ“ Ressources

### Documentation Officielle
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)

### Tutoriels
- [Spark by Examples](https://sparkbyexamples.com/)
- [Databricks Academy](https://academy.databricks.com/)
- [Learn Spark](https://developer.ibm.com/tutorials/spark-101/)

### Livres
- "Learning Spark" - Jules S. Damji et al.
- "Spark: The Definitive Guide" - Bill Chambers et al.

---

## ğŸ’¡ Conseils et Bonnes Pratiques

### 1. ArrÃªte les Sessions Properly
```python
try:
    # Ton code Spark
    pass
finally:
    spark.stop()
```

### 2. Utilise des DataFrames, Pas des RDDs
```python
# âœ… BON (DataFrame)
df.filter(F.col("age") > 25).show()

# âŒ MAUVAIS (RDD)
rdd = df.rdd.filter(lambda x: x["age"] > 25).collect()
```

### 3. Ã‰vite les Actions CoÃ»teuses
```python
# âŒ MAUVAIS: 2 actions
count = df.count()
results = df.collect()

# âœ… BON: 1 action
results = df.collect()
count = len(results)
```

### 4. Partitionne les DonnÃ©es Grandes
```python
# Ã‰crit partitionnÃ© pour requÃªtes rapides
df.write.partitionBy("year", "month").parquet("output/data")

# RequÃªte rapide (ne lit que year=2024/month=12/)
spark.read.parquet("output/data").filter(
    (F.col("year") == 2024) & (F.col("month") == 12)
).show()
```

---

## ğŸ“ Licence

MIT License - Voir [LICENSE](../LICENSE)

---

## ğŸ‘¨â€ğŸ“ Auteur

**Badr TAJINI**  
Data Engineering I - ESIEE 2025-2026

---

**DerniÃ¨re mise Ã  jour:** DÃ©cembre 2025  
**Statut:** âœ… ComplÃ©tÃ©  
**Version de Spark:** 3.0+  
**Version de Python:** 3.8+
