# ðŸ“š Concepts Fondamentaux de Spark

## 1. RDD (Resilient Distributed Dataset)

### CaractÃ©ristiques
- Abstraction bas niveau
- Immuable et distribuÃ©
- TolÃ©rant aux pannes
- Pas de schÃ©ma

### Exemple
```python
rdd = sc.parallelize([1, 2, 3, 4, 5])
result = rdd.map(lambda x: x * 2).collect()
# Output: [2, 4, 6, 8, 10]
```

---

## 2. DataFrame

### CaractÃ©ristiques
- Abstraction haut niveau
- Structure tabulaire (SQL-like)
- SchÃ©ma typÃ©
- OptimisÃ© par Catalyst

### Exemple
```python
df = spark.createDataFrame(
    [(1, "Alice"), (2, "Bob")],
    ["id", "name"]
)
df.show()
```

---

## 3. Transformations

### DÃ©finition
OpÃ©rations qui crÃ©ent un nouveau DataFrame/RDD sans l'exÃ©cuter immÃ©diatement.

### Types
- Narrow: map, filter (pas de shuffle)
- Wide: groupBy, join (avec shuffle)

### Exemples
```python
# Narrow
df.filter(F.col("age") > 25)
df.map(lambda x: x * 2)

# Wide
df.groupBy("city").count()
df.join(other_df, "id")
```

---

## 4. Actions

### DÃ©finition
OpÃ©rations qui retournent des rÃ©sultats au driver ou Ã©crivent sur disque.

### Exemples
```python
df.count()              # Retourne un nombre
df.show()               # Affiche les rÃ©sultats
df.collect()            # Retourne un tableau
df.write.parquet(path)  # Ã‰crit sur disque
```

---

## 5. DAG (Directed Acyclic Graph)

### Structure
```
[Source] â†’ [Transform 1] â†’ [Transform 2] â†’ [Action]
```

### Exemple
```python
df = spark.read.csv("data.csv")
df2 = df.filter(F.col("age") > 25)
df3 = df2.groupBy("city").count()
result = df3.show()  # DAG exÃ©cutÃ© ici

# DAG:
# Read CSV â†’ Filter â†’ GroupBy â†’ Show
```

