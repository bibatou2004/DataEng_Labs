# ğŸš€ Lab 2 Practice: PostgreSQL â†’ Star Schema ETL

**Data Engineering I - ESIEE 2025-2026**
**Auteur:** Badr TAJINI  
**Date:** DÃ©cembre 2025

---

## ğŸ“‹ Vue d'ensemble

Ce lab pratique implÃ©mente un **pipeline ETL complet** transformant une base de donnÃ©es opÃ©rationnelle PostgreSQL en un **data warehouse Star Schema** en utilisant **Apache Spark**.

### ğŸ¯ Objectifs

âœ… IngÃ©rer des donnÃ©es opÃ©rationnelles (CSV)  
âœ… Construire un schÃ©ma en Ã©toile (Star Schema)  
âœ… GÃ©nÃ©rer des clÃ©s de substitution stables  
âœ… Optimiser les joins et projections  
âœ… Valider la qualitÃ© des donnÃ©es  
âœ… Exporter en Parquet optimisÃ©  

---

## ğŸ“Š Architecture

### DonnÃ©es OpÃ©rationnelles (Bronze Layer)

```
customers           â†’ 10 clients
brands              â†’ 5 marques
categories          â†’ 5 catÃ©gories
products            â†’ 20 produits
orders              â†’ 50 commandes
order_items         â†’ 100 lignes de commande
```

### Dimensions (Silver Layer)

| Dimension | Lignes | Colonnes | ClÃ©s |
|-----------|--------|----------|------|
| **dim_customer** | 10 | 5 | customer_sk, customer_id |
| **dim_brand** | 5 | 3 | brand_sk, brand_id |
| **dim_category** | 5 | 3 | category_sk, category_id |
| **dim_product** | 20 | 6 | product_sk, product_id |
| **dim_date** | ~150 | 8 | date_sk, date |

### Table de Faits (Gold Layer)

```
fact_sales
â”œâ”€â”€ order_id (PK)
â”œâ”€â”€ date_sk (FK â†’ dim_date)
â”œâ”€â”€ customer_sk (FK â†’ dim_customer)
â”œâ”€â”€ product_sk (FK â†’ dim_product)
â”œâ”€â”€ quantity
â”œâ”€â”€ unit_price
â”œâ”€â”€ subtotal
â”œâ”€â”€ year (partition)
â””â”€â”€ month (partition)
```

**Statistiques:**
- 100 lignes de faits
- 50 commandes uniques
- GMV total: ~$20,000
- AOV moyen: ~$200

---

## ğŸ—ï¸ Structure du Projet

```
Lab2_Practice/
â”œâ”€â”€ README.md                           # Ce fichier
â”œâ”€â”€ requirements.txt                    # DÃ©pendances Python
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ lab2_practice.ipynb             # Notebook Jupyter principal (8 Ã©tapes)
â”‚
â”œâ”€â”€ data/                               # DonnÃ©es sources (Bronze)
â”‚   â”œâ”€â”€ lab2_customers.csv              # 10 clients
â”‚   â”œâ”€â”€ lab2_brands.csv                 # 5 marques
â”‚   â”œâ”€â”€ lab2_categories.csv             # 5 catÃ©gories
â”‚   â”œâ”€â”€ lab2_products.csv               # 20 produits
â”‚   â”œâ”€â”€ lab2_orders.csv                 # 50 commandes
â”‚   â””â”€â”€ lab2_order_items.csv            # 100 articles
â”‚
â”œâ”€â”€ outputs/lab2/                       # Sorties Parquet (Gold)
â”‚   â”œâ”€â”€ dim_customer/                   # Parquet
â”‚   â”œâ”€â”€ dim_brand/                      # Parquet
â”‚   â”œâ”€â”€ dim_category/                   # Parquet
â”‚   â”œâ”€â”€ dim_product/                    # Parquet
â”‚   â”œâ”€â”€ dim_date/                       # Parquet
â”‚   â””â”€â”€ fact_sales/                     # Parquet partitionnÃ© (year/month)
â”‚
â”œâ”€â”€ proof/                              # Preuves & MÃ©triques
â”‚   â”œâ”€â”€ plan_ingest.txt                 # Plan ingestion
â”‚   â”œâ”€â”€ ingestion_summary.csv           # Stats ingestion
â”‚   â”œâ”€â”€ dimensions_summary.csv          # Stats dimensions
â”‚   â”œâ”€â”€ date_dimension_summary.csv      # Stats dates
â”‚   â”œâ”€â”€ fact_sales_summary.csv          # Stats table de faits
â”‚   â”œâ”€â”€ plan_fact_join.txt              # Plan fact_sales
â”‚   â”œâ”€â”€ plan_case_a_late_projection.txt # Plan projection tardive
â”‚   â”œâ”€â”€ plan_case_b_early_projection.txt # Plan projection prÃ©coce
â”‚   â”œâ”€â”€ projection_comparison.csv       # Comparaison perfs
â”‚   â””â”€â”€ lab2_metrics_final.csv          # MÃ©triques finales
â”‚
â””â”€â”€ docs/                               # Documentation
    â”œâ”€â”€ ARCHITECTURE.md                 # DÃ©tails architecture
    â”œâ”€â”€ DATA_SCHEMA.md                  # SchÃ©mas dÃ©taillÃ©s
    â””â”€â”€ OPTIMIZATION_NOTES.md           # Notes d'optimisation
```

---

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis

```bash
# Python 3.8+
python --version

# PySpark 3.x
pip install pyspark>=3.0.0

# Pandas
pip install pandas
```

### Installation

```bash
# Clone le repo
git clone https://github.com/bibatou2004/DataEng_Labs.git
cd DataEng_Labs/Lab2_Practice

# Installe les dÃ©pendances
pip install -r requirements.txt

# Lance Jupyter
jupyter notebook notebooks/lab2_practice.ipynb
```

### ExÃ©cution

1. **Ouvre le notebook** dans Jupyter
2. **ExÃ©cute les cellules** dans l'ordre (Shift+Enter)
3. **Observe les rÃ©sultats** Ã  chaque Ã©tape
4. **VÃ©rifie les fichiers de preuve** dans `proof/`

---

## ğŸ“š DÃ©tails des Ã‰tapes

### Ã‰tape 0: Setup et SchÃ©mas
- Initialise Spark Session
- DÃ©finit schÃ©mas explicites pour tous les CSV
- CrÃ©e rÃ©pertoires de sortie

### Ã‰tape 1: Ingestion des DonnÃ©es
- Charge 6 tables CSV
- Affiche comptages et profils
- Sauvegarde plan d'ingestion

### Ã‰tape 2: Fonction ClÃ© de Substitution
```python
def sk(cols):
    return F.abs(F.xxhash64(*[F.col(c) for c in cols]))
```
- Hash stable 64-bit
- DÃ©terministe (mÃªme input = mÃªme output)
- Positif avec `abs()`

### Ã‰tape 3: Construction des Dimensions
- dim_customer (10 rows)
- dim_brand (5 rows)
- dim_category (5 rows)
- dim_product (20 rows, avec FKs)

### Ã‰tape 4: Dimension Date
- Extrait dates uniques des commandes
- GÃ©nÃ¨re attributs temporels (year, month, day, dow, quarter, week)
- ~150 jours sur 6 mois

### Ã‰tape 5: Table de Faits (Propre)
```python
# Ã‰TAPE 1: Joins
df_joined = (oi
    .join(p, F.col("oi.product_id") == F.col("p.product_id"))
    .join(o, F.col("oi.order_id") == F.col("o.order_id"))
    .join(c, F.col("o.customer_id") == F.col("c.customer_id"))
)

# Ã‰TAPE 2: Projection immÃ©diate (dÃ©sambiguation)
df_joined = df_joined.select(
    F.col("oi.order_id").alias("order_id"),
    ...
)

# Ã‰TAPE 3: Transformations
df_fact = df_joined.withColumn(...)
```

**Statistiques fact_sales:**
- 100 lignes de faits
- Mesures: quantity, unit_price, subtotal
- Partitions: year/month

### Ã‰tape 6: Export Parquet
```
dim_customer/ â”€â”€â†’ Parquet
dim_brand/    â”€â”€â†’ Parquet
dim_category/ â”€â”€â†’ Parquet
dim_product/  â”€â”€â†’ Parquet
dim_date/     â”€â”€â†’ Parquet
fact_sales/   â”€â”€â†’ Parquet (partitionnÃ© year/month)
```

### Ã‰tape 7: Optimisation - Projection Tardive vs PrÃ©coce

**Cas A: Projection Tardive** (JOIN â†’ AGG)
```python
(orders.join(order_items).join(products)
 .groupBy(...).agg(...))
```
- âŒ Traite toutes les colonnes
- âŒ Plus lent

**Cas B: Projection PrÃ©coce** (SELECT â†’ JOIN â†’ AGG)
```python
(orders.select("order_id", "order_date")
 .join(order_items.select("order_id", "product_id", "quantity"))
 .join(products.select("product_id", "price"))
 .groupBy(...).agg(...))
```
- âœ… Filtre les colonnes inutiles
- âœ… Plus rapide (surtout sur gros volumes)

**RÃ©sultats sur ce dataset:**
- Cas A (Tardive): ~X.XXXs
- Cas B (PrÃ©coce): ~X.XXXs
- AmÃ©lioration: ~Y%

### Ã‰tape 8: RÃ©sumÃ© Final
Affiche et sauvegarde:
- MÃ©triques clÃ©s (counts, GMV, AOV)
- Versions logicielles
- Fichiers de preuve gÃ©nÃ©rÃ©s

---

## ğŸ“Š MÃ©triques ClÃ©s

| MÃ©trique | Valeur |
|----------|--------|
| **Total Clients** | 10 |
| **Total Commandes** | 50 |
| **Total Articles** | 100 |
| **GMV Total** | ~$20,000 |
| **AOV Moyen** | ~$200 |
| **Dates Uniques** | ~150 |
| **Spark Version** | 3.x |

---

## ğŸ”§ DÃ©pannage

### Erreur: AMBIGUOUS_REFERENCE
**ProblÃ¨me:** Colonnes ambiguÃ«s dans les joins
```python
# âŒ Mauvais
.withColumn("customer_sk", sk(["customer_id"]))

# âœ… Bon
.withColumn("customer_sk", sk(["c.customer_id"]))
# OU projeter immÃ©diatement aprÃ¨s le join
```

### Erreur: File not found
```bash
# VÃ©rifie le chemin des donnÃ©es
ls -la data/lab2_*.csv

# Lance depuis le bon rÃ©pertoire
cd Lab2_Practice
```

### Spark Out of Memory
```bash
# Augmente la mÃ©moire driver
spark = SparkSession.builder \
    .config("spark.driver.memory", "16g") \
    .getOrCreate()
```

---

## ğŸ“ˆ Performances

### Optimisations AppliquÃ©es

1. **Projection PrÃ©coce**
   - RÃ©duit les donnÃ©es transmises entre les Ã©tapes
   - Permet Ã  Spark d'optimiser mieux

2. **Partitionnement des Sorties**
   - fact_sales partitionnÃ© par year/month
   - AccÃ©lÃ¨re les requÃªtes filtrÃ©es par date

3. **Parquet vs CSV**
   - Colonnaire (meilleure compression)
   - SchÃ©ma intÃ©grÃ©
   - Lecture sÃ©lective

### Mesures

Voir `proof/projection_comparison.csv`:
```csv
Cas,Approche,Temps(s),Lignes,AmÃ©lioration(%)
A,Projection Tardive,X.XXX,150,0.0
B,Projection PrÃ©coce,X.XXX,150,Y.Y
```

---

## ğŸ“š Concepts Couverts

### SQL & Spark
- âœ… Joins (inner, left, outer)
- âœ… Projections optimisÃ©es
- âœ… Window functions (rank)
- âœ… AgrÃ©gations (sum, avg, count)
- âœ… Partitionnement

### Data Engineering
- âœ… ETL pipeline
- âœ… Star Schema design
- âœ… Slowly Changing Dimensions
- âœ… Surrogate keys
- âœ… Data quality gates

### Spark Performance
- âœ… Query plans (DAG)
- âœ… Shuffle operations
- âœ… Broadcasting
- âœ… Columnar storage (Parquet)

---

## ğŸ“ Fichiers Importants

### Fichiers de Preuve
| Fichier | Contenu | Format |
|---------|---------|--------|
| `plan_ingest.txt` | Plan ingestion | TXT |
| `plan_fact_join.txt` | Plan fact_sales | TXT |
| `dimensions_summary.csv` | Stats dimensions | CSV |
| `fact_sales_summary.csv` | Stats table faits | CSV |
| `projection_comparison.csv` | Bench A vs B | CSV |
| `lab2_metrics_final.csv` | MÃ©triques finales | CSV |

### DonnÃ©es
- **Bronze:** `data/lab2_*.csv` (sources)
- **Gold:** `outputs/lab2/` (Parquet)

---

## ğŸ¤ Contribution

Pour amÃ©liorer ce lab:
1. Fork le repo
2. CrÃ©e une branche (`git checkout -b feature/improvement`)
3. Commit tes changements (`git commit -am 'Add improvement'`)
4. Push (`git push origin feature/improvement`)
5. Ouvre une Pull Request

---

## ğŸ“ Licence

MIT License - voir LICENSE

---

## ğŸ‘¨â€ğŸ« Auteur

**Badr TAJINI**  
Data Engineering I - ESIEE 2025-2026

---

## ğŸ“ Support

Questions? Ouvre une issue ou contacte via GitHub Discussions.

---

**DerniÃ¨re mise Ã  jour:** DÃ©cembre 2025  
**Status:** âœ… ComplÃ©tÃ© et TestÃ©
