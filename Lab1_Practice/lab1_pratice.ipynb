{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6aadd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 21:39:53 WARN Utils: Your hostname, Wandaogo, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/07 21:39:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 21:39:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Imports and Spark session\n",
    "import os, sys, datetime, pathlib\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"de1-lab1\").getOrCreate()\n",
    "print(\"Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4be9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 4\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n",
      "+---+-------+-----+\n",
      "|id |name   |value|\n",
      "+---+-------+-----+\n",
      "|1  |Alice  |10   |\n",
      "|2  |Bob    |20   |\n",
      "|3  |Charlie|30   |\n",
      "|4  |David  |40   |\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load the CSV inputs\n",
    "src_a = \"/home/bibawandaogo/data engineering 1/data/lab1_dataset_a.csv\"\n",
    "src_b = \"/home/bibawandaogo/data engineering 1/data/lab1_dataset_b.csv\"\n",
    "\n",
    "df_a = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_a)\n",
    "df_b = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_b)\n",
    "\n",
    "df = df_a.unionByName(df_b)\n",
    "df.cache()\n",
    "\n",
    "print(\"Rows:\", df.count())\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f7f759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes disponibles:\n",
      "['id', 'name', 'value']\n",
      "\n",
      "SchÃ©ma complet:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n",
      "\n",
      "PremiÃ¨res lignes:\n",
      "+---+-------+-----+\n",
      "| id|   name|value|\n",
      "+---+-------+-----+\n",
      "|  1|  Alice|   10|\n",
      "|  2|    Bob|   20|\n",
      "|  3|Charlie|   30|\n",
      "|  4|  David|   40|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VÃ©rifie les colonnes\n",
    "print(\"Colonnes disponibles:\")\n",
    "print(df.columns)\n",
    "print(\"\\nSchÃ©ma complet:\")\n",
    "df.printSchema()\n",
    "print(\"\\nPremiÃ¨res lignes:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5367b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n",
      "Spark: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Imports and Spark session\n",
    "import os, sys, datetime, pathlib\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"de1-lab1\").getOrCreate()\n",
    "print(\"Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91607000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File A exists: True\n",
      "File B exists: True\n",
      "\n",
      "âœ… Rows: 10\n",
      "\n",
      "âœ… Columns: ['id', 'category', 'value', 'text']\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+---+--------+-----+-----------------------------+\n",
      "|id |category|value|text                         |\n",
      "+---+--------+-----+-----------------------------+\n",
      "|1  |A       |100  |hello world spark programming|\n",
      "|2  |B       |200  |data engineering with pyspark|\n",
      "|3  |A       |150  |hello spark hello world      |\n",
      "+---+--------+-----+-----------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load the CSV inputs\n",
    "src_a = \"/home/bibawandaogo/data engineering 1/data/lab1_dataset_a.csv\"\n",
    "src_b = \"/home/bibawandaogo/data engineering 1/data/lab1_dataset_b.csv\"\n",
    "\n",
    "print(f\"File A exists: {os.path.exists(src_a)}\")\n",
    "print(f\"File B exists: {os.path.exists(src_b)}\")\n",
    "\n",
    "df_a = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_a)\n",
    "df_b = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_b)\n",
    "\n",
    "df = df_a.unionByName(df_b)\n",
    "df.cache()\n",
    "\n",
    "print(\"\\nâœ… Rows:\", df.count())\n",
    "print(\"\\nâœ… Columns:\", df.columns)\n",
    "df.printSchema()\n",
    "df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "647a1044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ”¤ Top 10 Tokens (RDD API)\n",
      "============================================================\n",
      "Token                     Count\n",
      "------------------------------------------------------------\n",
      "hello                         5\n",
      "spark                         4\n",
      "world                         4\n",
      "and                           3\n",
      "data                          3\n",
      "pyspark                       3\n",
      "big                           2\n",
      "dataframes                    2\n",
      "engineering                   2\n",
      "learning                      2\n",
      "============================================================\n",
      "âœ… Wrote outputs/top10_rdd.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Top-N with RDD API\n",
    "rdd = df.select(\"text\").rdd.flatMap(lambda row: (row[0] or \"\").lower().split())\n",
    "pair = rdd.map(lambda t: (t, 1))\n",
    "counts = pair.reduceByKey(lambda a,b: a+b)\n",
    "top_rdd = counts.sortBy(lambda kv: (-kv[1], kv[0])).take(10)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ”¤ Top 10 Tokens (RDD API)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Token':<20} {'Count':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for t, c in top_rdd:\n",
    "    print(f\"{t:<20} {c:>10}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save as CSV\n",
    "pathlib.Path(\"outputs\").mkdir(exist_ok=True)\n",
    "with open(\"outputs/top10_rdd.csv\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"token,count\\n\")\n",
    "    for t,c in top_rdd:\n",
    "        f.write(f\"{t},{c}\\n\")\n",
    "print(\"âœ… Wrote outputs/top10_rdd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415d2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved proof/plan_rdd.txt\n",
      "\n",
      "============================================================\n",
      "ðŸ“‹ RDD Execution Plan (first 300 chars):\n",
      "============================================================\n",
      "2025-12-07 21:46:11.736238\n",
      "\n",
      "InMemoryTableScan [id#301, category#302, value#303, text#304]\n",
      "   +- InMemoryRelation [id#301, category#302, value#303, text#304], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- Union\n",
      "            :- FileScan csv [id#301,category#302,value#303,text#304] Ba\n",
      "...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.5: RDD plan â€” evidence\n",
    "_ = counts.count()\n",
    "plan_rdd = df._jdf.queryExecution().executedPlan().toString()\n",
    "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
    "with open(\"proof/plan_rdd.txt\",\"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
    "    f.write(plan_rdd)\n",
    "print(\"âœ… Saved proof/plan_rdd.txt\")\n",
    "\n",
    "# Affiche les premiÃ¨res lignes du plan\n",
    "with open(\"proof/plan_rdd.txt\",\"r\") as f:\n",
    "    content = f.read()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ“‹ RDD Execution Plan (first 300 chars):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(content[:300])\n",
    "    print(\"...\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef19bbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ”¤ Top 10 Tokens (DataFrame API)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|token      |count|\n",
      "+-----------+-----+\n",
      "|hello      |5    |\n",
      "|spark      |4    |\n",
      "|world      |4    |\n",
      "|and        |3    |\n",
      "|data       |3    |\n",
      "|pyspark    |3    |\n",
      "|big        |2    |\n",
      "|dataframes |2    |\n",
      "|engineering|2    |\n",
      "|learning   |2    |\n",
      "+-----------+-----+\n",
      "\n",
      "============================================================\n",
      "âœ… Wrote outputs/top10_df.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Top-N with DataFrame API\n",
    "from pyspark.sql.functions import explode, split, lower, col, count, desc, asc\n",
    "\n",
    "tokens = explode(split(lower(col(\"text\")), \"\\\\s+\")).alias(\"token\")\n",
    "df_tokens = df.select(tokens).where(col(\"token\") != \"\")\n",
    "\n",
    "agg_df = df_tokens.groupBy(\"token\").agg(count(\"*\").alias(\"count\"))\n",
    "top_df = agg_df.orderBy(desc(\"count\"), asc(\"token\")).limit(10)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ”¤ Top 10 Tokens (DataFrame API)\")\n",
    "print(\"=\" * 60)\n",
    "top_df.show(truncate=False)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save as CSV\n",
    "top_df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"outputs/top10_df_tmp\")\n",
    "\n",
    "# Move single part file to stable path\n",
    "import glob, shutil\n",
    "part = glob.glob(\"outputs/top10_df_tmp/part*\")[0]\n",
    "shutil.copy(part, \"outputs/top10_df.csv\")\n",
    "print(\"âœ… Wrote outputs/top10_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef56ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved proof/plan_df.txt\n",
      "\n",
      "============================================================\n",
      "ðŸ“‹ DataFrame Execution Plan (first 300 chars):\n",
      "============================================================\n",
      "2025-12-07 21:46:42.402778\n",
      "\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=10, orderBy=[count#698L DESC NULLS LAST,token#697 ASC NULLS FIRST], output=[token#697,count#698L])\n",
      "   +- HashAggregate(keys=[token#697], functions=[count(1)], output=[token#697, count#698L])\n",
      "      +- Excha\n",
      "...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.5: DataFrame plan â€” evidence\n",
    "plan_df = top_df._jdf.queryExecution().executedPlan().toString()\n",
    "with open(\"proof/plan_df.txt\",\"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
    "    f.write(plan_df)\n",
    "print(\"âœ… Saved proof/plan_df.txt\")\n",
    "\n",
    "# Affiche les premiÃ¨res lignes du plan\n",
    "with open(\"proof/plan_df.txt\",\"r\") as f:\n",
    "    content = f.read()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ“‹ DataFrame Execution Plan (first 300 chars):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(content[:300])\n",
    "    print(\"...\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0cd19fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š PROJECTION EXPERIMENT\n",
      "============================================================\n",
      "\n",
      "ðŸ”´ Case A: select('*') then aggregate\n",
      "------------------------------------------------------------\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (9)\n",
      "+- HashAggregate (8)\n",
      "   +- Exchange (7)\n",
      "      +- HashAggregate (6)\n",
      "         +- InMemoryTableScan (1)\n",
      "               +- InMemoryRelation (2)\n",
      "                     +- Union (5)\n",
      "                        :- Scan csv  (3)\n",
      "                        +- Scan csv  (4)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [2]: [category#302, value#303]\n",
      "Arguments: [category#302, value#303]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [id#301, category#302, value#303, text#304], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [4]: [id#301, category#302, value#303, text#304]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/bibawandaogo/data engineering 1/data/lab1_dataset_a.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:int,text:string>\n",
      "\n",
      "(4) Scan csv \n",
      "Output [4]: [id#322, category#323, value#324, text#325]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/bibawandaogo/data engineering 1/data/lab1_dataset_b.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:int,text:string>\n",
      "\n",
      "(5) Union\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [2]: [category#302, value#303]\n",
      "Keys [1]: [category#302]\n",
      "Functions [1]: [partial_sum(value#303)]\n",
      "Aggregate Attributes [1]: [sum#1036L]\n",
      "Results [2]: [category#302, sum#1037L]\n",
      "\n",
      "(7) Exchange\n",
      "Input [2]: [category#302, sum#1037L]\n",
      "Arguments: hashpartitioning(category#302, 200), ENSURE_REQUIREMENTS, [plan_id=512]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [2]: [category#302, sum#1037L]\n",
      "Keys [1]: [category#302]\n",
      "Functions [1]: [sum(value#303)]\n",
      "Aggregate Attributes [1]: [sum(value#303)#975L]\n",
      "Results [2]: [category#302, sum(value#303)#975L AS sum_value#970L]\n",
      "\n",
      "(9) AdaptiveSparkPlan\n",
      "Output [2]: [category#302, sum_value#970L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ Case B: minimal projection then aggregate\n",
      "------------------------------------------------------------\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (9)\n",
      "+- HashAggregate (8)\n",
      "   +- Exchange (7)\n",
      "      +- HashAggregate (6)\n",
      "         +- InMemoryTableScan (1)\n",
      "               +- InMemoryRelation (2)\n",
      "                     +- Union (5)\n",
      "                        :- Scan csv  (3)\n",
      "                        +- Scan csv  (4)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [2]: [category#302, value#303]\n",
      "Arguments: [category#302, value#303]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [id#301, category#302, value#303, text#304], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [4]: [id#301, category#302, value#303, text#304]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/bibawandaogo/data engineering 1/data/lab1_dataset_a.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:int,text:string>\n",
      "\n",
      "(4) Scan csv \n",
      "Output [4]: [id#322, category#323, value#324, text#325]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/bibawandaogo/data engineering 1/data/lab1_dataset_b.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:int,text:string>\n",
      "\n",
      "(5) Union\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [2]: [category#302, value#303]\n",
      "Keys [1]: [category#302]\n",
      "Functions [1]: [partial_sum(value#303)]\n",
      "Aggregate Attributes [1]: [sum#1208L]\n",
      "Results [2]: [category#302, sum#1209L]\n",
      "\n",
      "(7) Exchange\n",
      "Input [2]: [category#302, sum#1209L]\n",
      "Arguments: hashpartitioning(category#302, 200), ENSURE_REQUIREMENTS, [plan_id=643]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [2]: [category#302, sum#1209L]\n",
      "Keys [1]: [category#302]\n",
      "Functions [1]: [sum(value#303)]\n",
      "Aggregate Attributes [1]: [sum(value#303)#1147L]\n",
      "Results [2]: [category#302, sum(value#303)#1147L AS sum_value#1144L]\n",
      "\n",
      "(9) AdaptiveSparkPlan\n",
      "Output [2]: [category#302, sum_value#1144L]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "âœ… Plans displayed above\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Projection experiment: select(\"*\") vs minimal projection\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ“Š PROJECTION EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Case A: select all columns then aggregate on 'category'\n",
    "print(\"\\nðŸ”´ Case A: select('*') then aggregate\")\n",
    "print(\"-\" * 60)\n",
    "all_cols = df.select(\"*\").groupBy(\"category\").agg(F.sum(\"value\").alias(\"sum_value\"))\n",
    "all_cols.explain(\"formatted\")\n",
    "_ = all_cols.count()  # trigger\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Case B: minimal projection then aggregate\n",
    "print(\"\\nðŸ”µ Case B: minimal projection then aggregate\")\n",
    "print(\"-\" * 60)\n",
    "proj = df.select(\"category\",\"value\").groupBy(\"category\").agg(F.sum(\"value\").alias(\"sum_value\"))\n",
    "proj.explain(\"formatted\")\n",
    "_ = proj.count()  # trigger\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Plans displayed above\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e226f091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Spark session stopped.\n",
      "============================================================\n",
      "ðŸŽ‰ Lab 1 Practice Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Cleanup\n",
    "spark.stop()\n",
    "print(\"\\nâœ… Spark session stopped.\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸŽ‰ Lab 1 Practice Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de1-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
