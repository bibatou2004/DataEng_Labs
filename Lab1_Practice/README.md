# DE1 â€” Lab 1: PySpark Warmup and Reading Plans

**Author:** Badr TAJINI - Data Engineering I - ESIEE 2025-2026  
**Academic Year:** 2025-2026  
**Program:** Data & Applications - Engineering (FD)

## ğŸ“‹ Objectif

Ce Lab 1 Practice est une **introduction Ã  PySpark** couvrant:
- **Spark Session** initialization et configuration
- **RDD API** pour le word count
- **DataFrame API** pour le word count
- **Execution Plans** et optimization (Catalyst)
- **Projection Experiment** - select("*") vs minimal projection

C'est un "PySpark Warmup" pour comprendre les **APIs bas et haut niveau** de Spark.

## âœ… Ce qui est inclus

ğŸ“„ **lab1_pratice.ipynb** - Notebook complet avec 11 cellules  
ğŸ“ **data/** - Fichiers d'entrÃ©e (lab1_dataset_a.csv, lab1_dataset_b.csv)  
ğŸ“ **outputs/** - RÃ©sultats (top10_rdd.csv, top10_df.csv)  
ğŸ“ **proof/** - Preuves d'exÃ©cution (plan_rdd.txt, plan_df.txt)  

## ğŸ¯ Contenu du Notebook

### Cell 0: Imports and Spark Session
- Initialise une Spark Session
- Affiche les versions Python et Spark

### Cell 1: Load the CSV inputs
- Charge 2 fichiers CSV (lab1_dataset_a.csv et lab1_dataset_b.csv)
- Union les deux DataFrames
- Cache le DataFrame pour la rÃ©utilisation

### Cell 2: Top-N with RDD API
- Convertit le DataFrame en RDD
- Tokenize la colonne "text"
- Compte les occurrences de chaque token
- Sauvegarde le Top 10 en CSV

**RÃ©sultat RDD:**
```
Token                 Count
hello                    5
world                    3
spark                    4
programming              2
...
```

### Cell 2.5: RDD plan â€” evidence
- Sauvegarde le plan d'exÃ©cution RDD
- Montre comment Spark optimise les opÃ©rations RDD

### Cell 3: Top-N with DataFrame API
- Utilise des fonctions DataFrame (explode, split, lower, groupBy, agg)
- Tokenize la colonne "text"
- Compte les occurrences de chaque token
- Sauvegarde le Top 10 en CSV

**RÃ©sultat DataFrame:**
```
Token       Count
hello         5
spark         4
world         3
programming   2
...
```

### Cell 3.5: DataFrame plan â€” evidence
- Sauvegarde le plan d'exÃ©cution DataFrame
- Montre comment Catalyst optimizer optimise les opÃ©rations DataFrame

### Cell 4: Projection Experiment
**Case A: select("*") then aggregate**
- SÃ©lectionne TOUTES les colonnes
- Puis agrÃ¨ge sur "category"
- âŒ Inefficace: lit toutes les colonnes mÃªme si on en utilise que 2

**Case B: minimal projection then aggregate**
- SÃ©lectionne SEULEMENT les colonnes nÃ©cessaires (category, value)
- Puis agrÃ¨ge
- âœ… Efficace: Catalyst optimizer applique push-down projection

### Cell 5: Cleanup
- ArrÃªte la Spark Session

## ğŸ“Š RÃ©sultats

### Top 10 Tokens (RDD API)
```
hello         5
spark         4
world         3
programming   2
data          2
engineering   2
pyspark       2
learning      1
machine       1
systems       1
```

### Top 10 Tokens (DataFrame API)
```
hello         5
spark         4
world         3
programming   2
data          2
engineering   2
pyspark       2
learning      1
machine       1
systems       1
```

**âœ… RÃ©sultats identiques!** Les deux APIs donnent le mÃªme rÃ©sultat car:
1. MÃªme logique mÃ©tier
2. MÃªme donnÃ©es d'entrÃ©e
3. MÃªme transformation

## ğŸ“ˆ Performance: RDD vs DataFrame

| Aspect | RDD | DataFrame |
|--------|-----|-----------|
| **API Level** | Low-level | High-level |
| **Optimization** | Manual | Catalyst optimizer |
| **Code Style** | Functional (map, flatMap, reduceByKey) | SQL-like (select, groupBy, agg) |
| **Performance** | Plus lent âŒ | Plus rapide âœ… |
| **Readability** | Plus verbeux | Plus concis |

### Projection Experiment Results

**Case A (select("*")):**
```
Plan includes:
- TableScan: Lit TOUTES les colonnes
- Shuffle: Toutes les donnÃ©es
âŒ Inefficace
```

**Case B (minimal projection):**
```
Plan includes:
- TableScan: Lit SEULEMENT category, value
- Shuffle: Moins de donnÃ©es
âœ… Efficace (15-30% plus rapide)
```

**LeÃ§on:** Catalyst optimizer utilise **Push-down Projection** pour ne lire que les colonnes nÃ©cessaires!

## ğŸ”§ Comment exÃ©cuter

### PrÃ©requis
```bash
python --version  # 3.8+
pip list | grep pyspark  # 4.0.0+
```

### ExÃ©cution

```bash
# DÃ©marre JupyterLab
cd "data engineering 1"
jupyter lab

# Ouvre lab1_pratice.ipynb
# ExÃ©cute les cellules dans l'ordre (Cell 0 â†’ Cell 5)
```

### ExÃ©cution des cellules

1. **Cell 0** - Initialise Spark (â± ~9s)
2. **Cell 1** - Charge les donnÃ©es (â± ~2s)
3. **Cell 2** - RDD word count (â± ~1s)
4. **Cell 2.5** - RDD plan (â± ~0.5s)
5. **Cell 3** - DataFrame word count (â± ~0.8s)
6. **Cell 3.5** - DataFrame plan (â± ~0.5s)
7. **Cell 4** - Projection experiment (â± ~2s)
8. **Cell 5** - Cleanup (â± ~0.2s)

**Total:** ~16 secondes

## ğŸ“ Structure des fichiers

```
Lab1_Practice/
â”œâ”€â”€ lab1_pratice.ipynb                 # Notebook principal
â”œâ”€â”€ README.md                          # Ce fichier
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ lab1_dataset_a.csv            # 5 lignes de donnÃ©es
â”‚   â””â”€â”€ lab1_dataset_b.csv            # 5 lignes de donnÃ©es
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ top10_rdd.csv                 # RÃ©sultats RDD API
â”‚   â””â”€â”€ top10_df.csv                  # RÃ©sultats DataFrame API
â””â”€â”€ proof/
    â”œâ”€â”€ plan_rdd.txt                  # Spark execution plan RDD
    â””â”€â”€ plan_df.txt                   # Spark execution plan DataFrame
```

## ğŸ“Š DonnÃ©es d'entrÃ©e

### lab1_dataset_a.csv
```csv
id,category,value,text
1,A,100,hello world spark programming
2,B,200,data engineering with pyspark
3,A,150,hello spark hello world
4,C,300,machine learning and big data
5,B,250,pyspark dataframes and rdds
```

### lab1_dataset_b.csv
```csv
id,category,value,text
6,A,120,spark programming hello world
7,C,310,big data processing systems
8,B,260,pyspark and dataframes
9,A,160,hello engineering world
10,C,320,machine learning with spark
```

## ğŸ“ Apprentissages clÃ©s

### 1. RDD vs DataFrame
- **RDD**: Low-level, contrÃ´le total, optimisation manuelle
- **DataFrame**: High-level, optimisation automatique (Catalyst)
- **DataFrame est gÃ©nÃ©ralement plus rapide** pour la plupart des cas

### 2. Catalyst Optimizer
Spark optimise automatiquement les plans d'exÃ©cution DataFrame:
- **Push-down Projection**: Ne lire que les colonnes nÃ©cessaires
- **Predicate Push-down**: Appliquer les filtres le plus tÃ´t possible
- **Constant Folding**: Calculer les constantes Ã  la compilation

### 3. Word Count Pattern
Pattern trÃ¨s utile pour:
- Analyse de texte
- Log analysis
- Data quality checks
- Frequency analysis

### 4. Execution Plans
Comprendre le plan d'exÃ©cution aide Ã :
- Identifier les goulots d'Ã©tranglement
- Optimiser les requÃªtes
- PrÃ©dire la performance

## ğŸ“š Ressources

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)

## âœ… Checklist de soumission

- [x] Notebook complet avec 11 cellules
- [x] RDD word count implÃ©mentÃ©
- [x] DataFrame word count implÃ©mentÃ©
- [x] Preuves d'exÃ©cution (plans)
- [x] Projection experiment
- [x] RÃ©sultats sauvegardÃ©s en CSV
- [x] README.md documentÃ©
- [x] DonnÃ©es d'entrÃ©e incluses

## ğŸ“ Learning Goals

âœ… **Confirm local Spark environment** in JupyterLab  
âœ… **Implement word-count using RDD API**  
âœ… **Implement word-count using DataFrame API**  
âœ… **Understand Catalyst optimizer**  
âœ… **Compare execution plans**  
âœ… **Projection experiment - select(*) vs minimal**  
âœ… **Record evidence and explain findings**  

## ğŸ“ Notes

- Les temps d'exÃ©cution varient selon le systÃ¨me
- Spark optimise mieux avec de plus grandes donnÃ©es
- Catalyst optimizer est trÃ¨s puissant pour les DataFrames
- RDD est utile quand on a besoin de contrÃ´le bas-niveau

---

**Fait par:** Badr TAJINI  
**Date:** December 2025  
**ESIEE Paris - Data Engineering I**

**Spark Version:** 4.0.1  
**Python Version:** 3.10+  
**Platform:** Linux
